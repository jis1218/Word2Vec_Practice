###딥러닝을 이용한 자연어 처리 입문 정리 (https://wikidocs.net/32829)

#### pandas
##### dataframe은 list, series, dict, numpy의 ndarrays 등으로 생성할 수 있음
```python
data = [
    ['1000', 'Steve', 90.72], 
    ['1001', 'James', 78.09], 
    ['1002', 'Doyeon', 98.43], 
    ['1003', 'Jane', 64.19], 
    ['1004', 'Pilwoong', 81.30],
    ['1005', 'Tony', 99.14],
]
df = pd.DataFrame(data)
print(df)
```
```
      0         1      2
0  1000     Steve  90.72
1  1001     James  78.09
2  1002    Doyeon  98.43
3  1003      Jane  64.19
4  1004  Pilwoong  81.30
5  1005      Tony  99.14
```

##### dataframe 조회하기
##### df.head(n) - 앞 부분을 n개만 보기, df.head()만 하면 모든 테이블 값이 나오게 된다.
##### df.tail(n) - 뒷 부분을 n개만 보기, 역시 df.tail()만 하면 모든 테이블 값이 나오게 된다.
##### df['name'] - name에 해당하는 row를 본다.

##### 머신러닝 워크플로우
##### 1. 수집 - 기계학습 시켜야 할 데이터가 필요한데 자연어 데이터의 경우 corpus라고 부른다.
##### corpus의 파일 형식은 txt, csv, xml 등이 있다.

##### 2. 점검 및 탐색 - 데이터의 구조, 노이즈 데이터, 머신 러닝 적용을 위해 데이터를 어떻게 정제할지 정한다.
##### 이 단계를 Exploratory Data Analysis, EDA 단계라고도 하는데 데이터의 특징과 내재하는 구조적 관계를 알아내는 과정을 의미, 시각화와 통계 테스트 수반

##### 3. 전처리 및 정제 - 자연어 처리의 경우 토큰화, 정제, 정규화 ,불용어(stopwords) 제거 등의 단계를 포함
##### 까다로운 전처리의 경우 머신 러닝이 사용되기도 함

##### 4. 모델링 및 훈련 - 머신 러닝에 대한 코드를 작성하는 단계
##### 전처리가 완료된 데이터를 모델링이 끝난 머신러닝 알고리즘에 학습시킴
##### 모든 데이터를 사용하면 안되고 훈련 데이터, 테스트 데이터를 나누어야 함, 그래야 기계 학습 후 성능이 얼마나 되는지를 측정할 수 있으며 Overfitting 되는 것을 막을 수 있음

##### 5. 평가 - 테스트용 데이터로 평가

##### 6. 배포 - 완성된 모델 배포

### 텍스트 전처리(Text preprocessing) - 용도에 맞게 텍스트를 사전에 분류하는 작업
#### 토큰화(Tokenization)
##### corpus에서 token이라 불리는 단위로 나누는 작업을 tokenization이라고 한다. 토큰의 단위는 보통 의미있는 단위로 정한다.
##### 1. 단어 토큰화(Word Tokenization)
##### tokenization의 예시
```
입력: Time is an illusion. Lunchtime double so! 
출력 : "Time", "is", "an", "illustion", "Lunchtime", "double", "so"
```
##### 위의 예시는 간단한 토큰화 작업으로 구두점을 지운 뒤 띄어쓰기를 기준으로 잘라냄(정제 작업이라고 할 수 있겠음)
##### 하지만 간혹 구두점이나 특수문자를 제거하면 token이 의미를 잃어버리는 경우가 있고 띄어쓰기로 단위를 자르면 한글의 경우 단어 토큰을 구분하기 어렵다.

##### 2. 토큰화의 기준을 어떻게 잡아야 할까?
##### 특히 영어의 '가 들어간 경우는 어떻게 토큰화 할것인가? - 기준은 해당 데이터를 어떻게 사용할 것인가에 따라 나누면 되겠다.
##### 결국 토큰화의 기준은 어떻게 사용할 것인지가 중요하게 작용한다.

##### 3. 토큰화에서 고려해야할 사항
##### 1) 구두점이나 특수 문자를 단순 제외해서는 안된다.
##### 온점(.)의 경우 문장의 경계를 알 수 있는데 도움이 되므로 함부로 제외하긴 어렵고
##### $35라는 단어가 있을 때에도 $는 돈을 뜻하기 때문에 제외하기 어렵다.
##### 2) 줄임말, 띄어쓰기가 있는 경우
##### we're -> we are, New York은 한 단어이지만 띄어쓰기 존재

##### 4. 문장 토큰화(Sentence Tokenization)
##### 토큰의 단위를 문장으로 설정할 때 어떻게 나눌 수 있을까?
##### 온점으로 정의한다면 IP 주소 등이 나오는 문장에서는 적절치 않을 것이다.
##### NLTK의 sent_tokenize를 이용하면 문장 중간에 온점이 있더라도 적절하게 tokenization을 할 수 있다.

##### 5. 이진 분류기
##### 이진 분류기로 온점이 어느 클래스에 속하는지 분류하는 이진 분류기가 필요하다.

##### 6. 한국어 토큰화
##### 한국어는 띄어쓰기만으로 토큰화 하기 어려움. 영어는 단어별로 띄어쓰기가 되어 있지만 한국어는 '사람은' 이라는 단어 안에 '사람'이라는 명사와 '은'이라는 조사가 있기 때문에...
##### 한국어에서 띄어쓰기 단위가 되는 단위를 어절이라고 하는데 어절 토큰화는 한국어 NLP에서 지양되고 있음

##### 7. 품사 부착
##### 단어 표기는 같으나 품사에 따라 단어의 의미가 달라지기도 함 ex) 못(nail, not)
##### 그래서 토큰화 과정에서 각 단어가 어떤 품사로 쓰였는지 구분하기도 하는데 이를 Part-of-speech tagging 이라고 함

#### 정제(Cleaning) 및 정규화(Normalization)
##### 토큰화 작업 후에는 텍스트 데이터를 용도에 맞게 정제 및 정규화 한다.
##### 정제 : 코퍼스로부터 노이즈 제거
##### 정규화 : 표현 방법이 다른 단어를 통합시켜 같은 단어로 만든다.

##### 정제 작업은 토큰화 작업을 원활하게 하기 위해 하기도 하지만 토큰화 이후에도 노이즈를 제거하기 위해 하기도 한다.

##### 1. 규칙에 기반한 표기가 다른 단어들의 통합
##### 2. 대 소문자 통합
##### 영어의 경우 단어의 수를 줄여줄 수 있다. 하지만 무작정 하면 안되는데 미국(US)와 us(우리)는 구별되어야 하기 때문, 사람 이름 등도 대문자로 시작하는게 좋다.
##### 3. 불필요한 단어의 제거
##### 노이즈는 자연어가 아니면서 아무 의미도 갖지 않는 글자들을 의미하기도 하나 분석하고자 하는 목적에 맞지 않는 불필요한 단어들도 될 수 있다.
##### 불필요한 단어를 제거하는 방법으로는 3가지가 있는데 불용어 제거와 등장 빈도가 적은 단어, 길이가 짧은 단어를 제거하는 방법이 있음, 길이가 짧은 단어는 보통 불용어에 해당함








##### Why Word2Vec?

##### Vector can represent each word. Similar words tend to have similar vector. We can measure it by cosine similarity and so on. Word vectors encode valuable semantic information about the words that they represent.

##### There are two main Word2Vec models: Continuous Bag of Words and Skip-Gram.
##### CBOW : predict a word given a context
##### Skip-Gram : predict a context given a word