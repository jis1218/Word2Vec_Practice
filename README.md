##### Why Word2Vec?

##### Vector can represent each word. Similar words tend to have similar vector. We can measure it by cosine similarity and so on. Word vectors encode valuable semantic information about the words that they represent.

##### There are two main Word2Vec models: Continuous Bag of Words and Skip-Gram.
##### CBOW : predict a word given a context
##### Skip-Gram : predict a context given a word